{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbbaik/small_bert/blob/main/Small_BERT_Korean_Pre_training_Consolidated_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import math\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from datasets import load_dataset # Hugging Face datasets 라이브러리 임포트\n",
        "\n",
        "# --- 1. Colab 환경 설정 및 구글 드라이브 마운트 ---\n",
        "# 구글 드라이브 마운트\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 드라이브에 저장할 경로 설정\n",
        "SAVE_PATH = '/content/drive/MyDrive/small_bert_korean'\n",
        "\n",
        "# 디렉토리가 없으면 생성\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH)\n",
        "    print(f\"디렉토리 '{SAVE_PATH}'가 생성되었습니다.\")\n",
        "\n",
        "# --- 2. 하이퍼파라미터 정의 ---\n",
        "d_model = 768  # 일반 BERT-base와 동일한 임베딩 차원\n",
        "n_head = 12    # 일반 BERT-base와 동일한 헤드 수\n",
        "num_layers = 6 # BERT-base의 절반\n",
        "dim_feedforward = d_model * 4\n",
        "max_len = 50\n",
        "dropout = 0.1\n",
        "\n",
        "# --- 3. Positional Encoding 클래스 정의 ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # math.log를 사용하기 위해 math 모듈이 임포트되어 있어야 합니다.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# --- 4. SmallBERT 모델 클래스 정의 ---\n",
        "class SmallBERT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, num_layers, dim_feedforward, max_len, dropout):\n",
        "        super(SmallBERT, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_head,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # 새로운 선형 레이어 추가: d_model -> vocab_size로 차원을 변환\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch_size, seq_len)\n",
        "        x = self.embedding(src) * math.sqrt(d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        output = self.transformer_encoder(x)\n",
        "\n",
        "        # 마지막에 output_layer를 통과시켜 vocab_size 차원으로 변환\n",
        "        output = self.output_layer(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# --- 5. 한국어 데이터셋 준비 (NSMC 데이터셋 사용) ---\n",
        "print(\"네이버 영화 리뷰 감성분석 데이터셋(NSMC)을 로드합니다...\")\n",
        "# NSMC 데이터셋 로드\n",
        "# 'train' 스플릿만 사용 (사전 학습용)\n",
        "nsmc_dataset = load_dataset('nsmc', split='train')\n",
        "print(f\"NSMC 데이터셋 로드 완료. 총 {len(nsmc_dataset)}개의 샘플.\")\n",
        "\n",
        "# 데이터셋 클래스 정의 (Hugging Face Dataset 객체를 직접 받도록 수정)\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, tokenizer, block_size=50):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.examples = []\n",
        "\n",
        "        # Hugging Face Dataset의 'document' 필드에서 텍스트를 추출하여 토큰화\n",
        "        for i, entry in enumerate(hf_dataset):\n",
        "            text = entry['document']\n",
        "            if text is None: # None 값 처리\n",
        "                continue\n",
        "            tokenized_text = self.tokenizer.encode(text)\n",
        "            token_ids = tokenized_text.ids\n",
        "\n",
        "            # 텍스트를 고정된 길이의 블록으로 분할\n",
        "            for j in range(0, len(token_ids) - block_size + 1, block_size):\n",
        "                self.examples.append(torch.tensor(token_ids[j:j+block_size], dtype=torch.long))\n",
        "\n",
        "        print(f\"TextDataset에 {len(self.examples)}개의 블록이 준비되었습니다.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.examples[i]\n",
        "\n",
        "# --- 6. BPE 기반 한국어 토크나이저 학습 및 저장 ---\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "# NSMC 데이터셋의 'document' 필드에서 텍스트를 추출하여 토크나이저 학습에 사용\n",
        "# generator를 사용하여 메모리 효율적으로 텍스트를 전달\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(nsmc_dataset), 1000): # 대규모 데이터셋의 경우 배치로 처리\n",
        "        yield [text for text in nsmc_dataset[i : i + 1000]['document'] if text is not None]\n",
        "\n",
        "print(\"토크나이저를 학습합니다...\")\n",
        "tokenizer.train_from_iterator(\n",
        "    get_training_corpus(),\n",
        "    vocab_size=30000,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "# 학습된 토크나이저를 드라이브에 저장\n",
        "tokenizer.save_model(SAVE_PATH)\n",
        "print(f\"토크나이저가 '{SAVE_PATH}'에 저장되었습니다.\")\n",
        "\n",
        "# --- 7. 모델 인스턴스 생성 및 사전 학습 루프 ---\n",
        "vocab_size = tokenizer.get_vocab_size() # 토크나이저 학습 후 실제 vocab_size 가져오기\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SmallBERT(vocab_size, d_model, n_head, num_layers, dim_feedforward, max_len, dropout).to(device)\n",
        "\n",
        "# 데이터로더 설정 (NSMC 데이터셋을 TextDataset에 전달)\n",
        "dataset_for_training = TextDataset(nsmc_dataset, tokenizer, block_size=max_len)\n",
        "dataloader = DataLoader(dataset_for_training, batch_size=32, shuffle=True)\n",
        "\n",
        "# 옵티마이저 및 손실 함수\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"[PAD]\")) # PAD 토큰은 손실 계산에서 제외\n",
        "\n",
        "# 사전 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        inputs = batch.to(device)\n",
        "\n",
        "        # MLM을 위한 마스킹\n",
        "        labels = inputs.clone().detach()\n",
        "        # 15%의 토큰을 마스킹\n",
        "        masked_indices = torch.rand(inputs.shape) < 0.15\n",
        "        # [CLS], [SEP], [PAD] 토큰은 마스킹하지 않도록 예외 처리\n",
        "        masked_indices[inputs == tokenizer.token_to_id(\"[CLS]\")] = False\n",
        "        masked_indices[inputs == tokenizer.token_to_id(\"[SEP]\")] = False\n",
        "        masked_indices[inputs == tokenizer.token_to_id(\"[PAD]\")] = False\n",
        "\n",
        "        inputs[masked_indices] = tokenizer.token_to_id(\"[MASK]\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # 마스킹된 토큰에 대한 손실 계산\n",
        "        # outputs: (batch_size * seq_len, vocab_size)\n",
        "        # labels: (batch_size * seq_len)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Perplexity: {math.exp(avg_loss):.4f}\")\n",
        "\n",
        "    # 중간 체크포인트 저장 (구글 드라이브)\n",
        "    checkpoint_path = os.path.join(SAVE_PATH, f'model_checkpoint_epoch_{epoch+1}.pt')\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"모델 체크포인트가 {checkpoint_path}에 저장되었습니다.\")\n",
        "\n",
        "# --- 8. 최종 모델 및 성능 지표 저장 ---\n",
        "final_model_path = os.path.join(SAVE_PATH, 'final_small_bert.pt')\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"최종 모델이 {final_model_path}에 저장되었습니다.\")\n",
        "\n",
        "performance_log_path = os.path.join(SAVE_PATH, 'training_performance.txt')\n",
        "with open(performance_log_path, 'w') as f:\n",
        "    f.write(f\"Final Average Loss: {avg_loss}\\n\")\n",
        "    f.write(f\"Final Perplexity: {math.exp(avg_loss)}\\n\")\n",
        "    f.write(f\"Total Epochs: {num_epochs}\\n\")\n",
        "print(f\"학습 성능 지표가 {performance_log_path}에 저장되었습니다.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "DE5_xyK3s24F"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}