{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbbaik/small_bert/blob/main/small_bert_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 구글 드라이브 마운트\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 드라이브에 저장할 경로 설정 (미리 폴더를 생성해두는 것이 좋습니다)\n",
        "SAVE_PATH = '/content/drive/MyDrive/small_bert_korean'"
      ],
      "metadata": {
        "id": "HCjrjq8KSKy4",
        "outputId": "b2b5fe04-7ce8-46f6-94a8-1f936452a56d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "CU7YdB7ISXOB",
        "outputId": "8f22c01d-db9f-4e5f-906e-e036b90756fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 하이퍼파라미터 정의\n",
        "d_model = 768  # 일반 BERT-base와 동일한 임베딩 차원\n",
        "n_head = 12    # 일반 BERT-base와 동일한 헤드 수\n",
        "num_layers = 6 # BERT-base의 절반\n",
        "dim_feedforward = d_model * 4\n",
        "max_len = 50\n",
        "vocab_size = 30000  # 예시로 설정한 어휘 크기\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "fcu_FIsPTHaw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "Q-1e9BVfTvbm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정된 SmallBERT 클래스\n",
        "class SmallBERT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, num_layers, dim_feedforward, max_len, dropout):\n",
        "        super(SmallBERT, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_head,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # 새로운 선형 레이어 추가: d_model -> vocab_size로 차원을 변환\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch_size, seq_len)\n",
        "        x = self.embedding(src) * math.sqrt(d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        output = self.transformer_encoder(x)\n",
        "\n",
        "        # 마지막에 output_layer를 통과시켜 vocab_size 차원으로 변환\n",
        "        output = self.output_layer(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "XJUgI4PsTyJv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SmallBERT(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    n_head=n_head,\n",
        "    num_layers=num_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    max_len=max_len,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "X6gqsPRgTx5M",
        "outputId": "b61e7613-4bc2-43d3-bf5d-581571ead347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SmallBERT(\n",
            "  (embedding): Embedding(30000, 768)\n",
            "  (pos_encoder): PositionalEncoding()\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 구글 드라이브 마운트\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 저장할 경로 설정\n",
        "SAVE_PATH = '/content/drive/MyDrive/small_bert_korean'\n",
        "\n",
        "# 디렉토리가 없으면 생성\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH)\n",
        "    print(f\"디렉토리 '{SAVE_PATH}'가 생성되었습니다.\")\n",
        "\n",
        "# 이제 파일을 저장하는 코드를 실행\n",
        "text_data = \"안녕하세요. 저는 작은 bert 모델을 만들고 있습니다. 자연어 처리는 정말 재미있습니다. 한국어 데이터로 사전 학습을 진행합니다.\" * 1000\n",
        "with open(os.path.join(SAVE_PATH, 'korean_text.txt'), 'w', encoding='utf-8') as f:\n",
        "    f.write(text_data)\n",
        "\n",
        "print(\"파일이 성공적으로 생성되었습니다.\")"
      ],
      "metadata": {
        "id": "olDCiH97TSas",
        "outputId": "5465ea2a-8e7b-4402-906c-11559cd4ff2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "디렉토리 '/content/drive/MyDrive/small_bert_korean'가 생성되었습니다.\n",
            "파일이 성공적으로 생성되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 텍스트 파일 생성\n",
        "# 실제로는 대용량 한국어 텍스트 파일을 사용해야 합니다.\n",
        "text_data = \"안녕하세요. 저는 작은 bert 모델을 만들고 있습니다. 자연어 처리는 정말 재미있습니다. 한국어 데이터로 사전 학습을 진행합니다.\" * 1000\n",
        "with open(os.path.join(SAVE_PATH, 'korean_text.txt'), 'w', encoding='utf-8') as f:\n",
        "    f.write(text_data)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=50):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.examples = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        tokenized_text = self.tokenizer.encode(text)\n",
        "        token_ids = tokenized_text.ids\n",
        "\n",
        "        # 텍스트를 고정된 길이의 블록으로 분할\n",
        "        for i in range(0, len(token_ids) - block_size + 1, block_size):\n",
        "            self.examples.append(torch.tensor(token_ids[i:i+block_size], dtype=torch.long))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.examples[i]"
      ],
      "metadata": {
        "id": "mjcOjpFOStnp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 학습 및 저장\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "files = [os.path.join(SAVE_PATH, 'korean_text.txt')]\n",
        "\n",
        "tokenizer.train(\n",
        "    files,\n",
        "    vocab_size=30000,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "# 학습된 토크나이저를 드라이브에 저장\n",
        "tokenizer.save_model(SAVE_PATH)"
      ],
      "metadata": {
        "id": "8Z3lJvPsTAfm",
        "outputId": "86ec959b-a816-4dc5-87f3-70d3290c1c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/small_bert_korean/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 하이퍼파라미터 정의\n",
        "d_model = 768\n",
        "n_head = 12\n",
        "num_layers = 6\n",
        "dim_feedforward = d_model * 4\n",
        "max_len = 50\n",
        "dropout = 0.1\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "# 모델 클래스 (이전 응답에서 정의한 내용과 동일)\n",
        "# PositionalEncoding 및 SmallBERT 클래스 코드를 여기에 다시 붙여넣기\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SmallBERT(vocab_size, d_model, n_head, num_layers, dim_feedforward, max_len, dropout).to(device)\n",
        "\n",
        "# 데이터로더 설정\n",
        "dataset = TextDataset(os.path.join(SAVE_PATH, 'korean_text.txt'), tokenizer, block_size=max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 옵티마이저 및 손실 함수\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 사전 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        inputs = batch.to(device)\n",
        "\n",
        "        # MLM을 위한 마스킹\n",
        "        labels = inputs.clone().detach()\n",
        "        masked_indices = torch.rand(inputs.shape) < 0.15\n",
        "        inputs[masked_indices] = tokenizer.token_to_id(\"[MASK]\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # 마스킹된 토큰에 대한 손실 계산\n",
        "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Perplexity: {math.exp(avg_loss):.4f}\")\n",
        "\n",
        "    # 중간 체크포인트 저장 (구글 드라이브)\n",
        "    checkpoint_path = os.path.join(SAVE_PATH, f'model_checkpoint_epoch_{epoch+1}.pt')\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"모델 체크포인트가 {checkpoint_path}에 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "zXB6FXGNTDlf",
        "outputId": "b9223c39-dabf-4fbb-a863-4bf2fc7c57b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3719, Perplexity: 3.9428\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_1.pt에 저장되었습니다.\n",
            "Epoch [2/10], Loss: 0.4445, Perplexity: 1.5597\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_2.pt에 저장되었습니다.\n",
            "Epoch [3/10], Loss: 0.4155, Perplexity: 1.5151\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_3.pt에 저장되었습니다.\n",
            "Epoch [4/10], Loss: 0.4033, Perplexity: 1.4968\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_4.pt에 저장되었습니다.\n",
            "Epoch [5/10], Loss: 0.4022, Perplexity: 1.4951\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_5.pt에 저장되었습니다.\n",
            "Epoch [6/10], Loss: 0.3918, Perplexity: 1.4797\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_6.pt에 저장되었습니다.\n",
            "Epoch [7/10], Loss: 0.3891, Perplexity: 1.4756\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_7.pt에 저장되었습니다.\n",
            "Epoch [8/10], Loss: 0.3721, Perplexity: 1.4508\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_8.pt에 저장되었습니다.\n",
            "Epoch [9/10], Loss: 0.3584, Perplexity: 1.4311\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_9.pt에 저장되었습니다.\n",
            "Epoch [10/10], Loss: 0.3673, Perplexity: 1.4438\n",
            "모델 체크포인트가 /content/drive/MyDrive/small_bert_korean/model_checkpoint_epoch_10.pt에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 모델 저장\n",
        "final_model_path = os.path.join(SAVE_PATH, 'final_small_bert.pt')\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"최종 모델이 {final_model_path}에 저장되었습니다.\")\n",
        "\n",
        "# 학습 성능 지표 저장 (예시)\n",
        "performance_log_path = os.path.join(SAVE_PATH, 'training_performance.txt')\n",
        "with open(performance_log_path, 'w') as f:\n",
        "    f.write(f\"Final Average Loss: {avg_loss}\\n\")\n",
        "    f.write(f\"Final Perplexity: {math.exp(avg_loss)}\\n\")\n",
        "    f.write(f\"Total Epochs: {num_epochs}\\n\")\n",
        "print(f\"학습 성능 지표가 {performance_log_path}에 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "UHkl4rGqTEnQ",
        "outputId": "9d17651f-c348-469f-e08e-c5adc8c7fee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 모델이 /content/drive/MyDrive/small_bert_korean/final_small_bert.pt에 저장되었습니다.\n",
            "학습 성능 지표가 /content/drive/MyDrive/small_bert_korean/training_performance.txt에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwhLlmyKTsTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colab 시작하기",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}