{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBmDGf/qqTktlcI1QxSQZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbbaik/small_bert/blob/main/predict_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVvHuom5ZOJc",
        "outputId": "f616b27a-1a17-4696-e48d-7b03524cdba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SmallBERT(\n",
              "  (embedding): Embedding(114, 768)\n",
              "  (pos_encoder): PositionalEncoding()\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output_layer): Linear(in_features=768, out_features=114, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import os\n",
        "import math\n",
        "\n",
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 저장 경로 설정\n",
        "SAVE_PATH = '/content/drive/MyDrive/small_bert_korean'\n",
        "\n",
        "# 하이퍼파라미터 정의 (학습 시와 동일하게 설정)\n",
        "d_model = 768\n",
        "n_head = 12\n",
        "num_layers = 6\n",
        "dim_feedforward = d_model * 4\n",
        "max_len = 50\n",
        "dropout = 0.1\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = BertWordPieceTokenizer.from_file(\n",
        "    os.path.join(SAVE_PATH, 'vocab.txt'),\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "# 모델 클래스 정의 (학습 시 사용했던 SmallBERT 클래스를 다시 정의해야 합니다)\n",
        "# 이전에 제공된 SmallBERT 클래스 코드를 여기에 그대로 붙여넣습니다.\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SmallBERT(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, num_layers, dim_feedforward, max_len, dropout):\n",
        "        super(SmallBERT, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_head, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output_layer = torch.nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, src):\n",
        "        x = self.embedding(src) * math.sqrt(d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        output = self.transformer_encoder(x)\n",
        "        output = self.output_layer(output)\n",
        "        return output\n",
        "\n",
        "# 모델 인스턴스 생성 및 학습된 가중치 로드\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SmallBERT(vocab_size, d_model, n_head, num_layers, dim_feedforward, max_len, dropout).to(device)\n",
        "model.load_state_dict(torch.load(os.path.join(SAVE_PATH, 'final_small_bert.pt'), map_location=device))\n",
        "model.eval()  # 추론 모드로 전환"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_masked_token(text, model, tokenizer, device, top_k=5):\n",
        "    # 입력 텍스트 토큰화\n",
        "    tokenized_input = tokenizer.encode(text)\n",
        "    input_ids = tokenized_input.ids\n",
        "\n",
        "    # [MASK] 토큰의 인덱스 찾기\n",
        "    mask_token_id = tokenizer.token_to_id(\"[MASK]\")\n",
        "    mask_idx = input_ids.index(mask_token_id)\n",
        "\n",
        "    # 텐서로 변환\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # 모델에 입력하여 예측\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "\n",
        "    # 마스크 위치의 로짓(logits)만 추출\n",
        "    mask_logits = outputs[0, mask_idx]\n",
        "\n",
        "    # 상위 k개의 토큰 예측\n",
        "    top_k_values, top_k_indices = torch.topk(mask_logits, top_k)\n",
        "\n",
        "    predicted_tokens = [tokenizer.id_to_token(idx.item()) for idx in top_k_indices]\n",
        "\n",
        "    print(f\"원문: {text}\")\n",
        "    print(f\"[MASK] 토큰 예측 결과 (Top {top_k}):\")\n",
        "    for i, token in enumerate(predicted_tokens):\n",
        "        print(f\"{i+1}. {token} (확률: {torch.softmax(mask_logits, dim=-1)[top_k_indices[i]].item():.4f})\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "Bj1QyFgDZXEM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 예시 1\n",
        "text_to_predict_1 = \"나는 [MASK]을 먹고싶다.\"\n",
        "predict_masked_token(text_to_predict_1, model, tokenizer, device)\n",
        "\n",
        "# 예측 예시 2\n",
        "text_to_predict_2 = \"콜랩 환경에서 딥러닝 모델을 [MASK]하는 방법.\"\n",
        "predict_masked_token(text_to_predict_2, model, tokenizer, device)\n",
        "\n",
        "# 예측 예시 3\n",
        "text_to_predict_3 = \"오늘 날씨는 [MASK] 맑다.\"\n",
        "predict_masked_token(text_to_predict_3, model, tokenizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUVUU29fZXWW",
        "outputId": "670d97a5-6d68-42f0-aabb-c681158c497d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문: 나는 [MASK]을 먹고싶다.\n",
            "[MASK] 토큰 예측 결과 (Top 5):\n",
            "1. 저는 (확률: 0.2456)\n",
            "2. 처리는 (확률: 0.1202)\n",
            "3. 학습을 (확률: 0.1196)\n",
            "4. 모델을 (확률: 0.0691)\n",
            "5. 정말 (확률: 0.0577)\n",
            "--------------------------------------------------\n",
            "원문: 콜랩 환경에서 딥러닝 모델을 [MASK]하는 방법.\n",
            "[MASK] 토큰 예측 결과 (Top 5):\n",
            "1. 저는 (확률: 0.1686)\n",
            "2. bert (확률: 0.1508)\n",
            "3. 학습을 (확률: 0.0984)\n",
            "4. 안녕하세요 (확률: 0.0746)\n",
            "5. 있습니다 (확률: 0.0603)\n",
            "--------------------------------------------------\n",
            "원문: 오늘 날씨는 [MASK] 맑다.\n",
            "[MASK] 토큰 예측 결과 (Top 5):\n",
            "1. 저는 (확률: 0.3069)\n",
            "2. 학습을 (확률: 0.0938)\n",
            "3. 처리는 (확률: 0.0797)\n",
            "4. 사전 (확률: 0.0706)\n",
            "5. 모델을 (확률: 0.0568)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJc4me28Zb5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}